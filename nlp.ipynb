{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized sentences:\n",
      "Tokenization is the process of breaking down text into words or sentences.\n",
      "It is an essential step in natural language processing.\n",
      "This code demonstrates how to tokenize text using Python's NLTK library.\n",
      "\n",
      "Tokenized words:\n",
      "['tokenization', 'process', 'breaking', 'text', 'words', 'sentences', '', 'essential', 'step', 'natural', 'language', 'processing', '', 'code', 'demonstrates', 'tokenize', 'text', 'using', 'python', 'nltk', 'library', '']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "# Sample text for tokenization\n",
    "text = \"Tokenization is the process of breaking down text into words or sentences. \\\n",
    "        It is an essential step in natural language processing. \\\n",
    "        This code demonstrates how to tokenize text using Python's NLTK library.\"\n",
    "\n",
    "# Tokenize the text into sentences\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "# Tokenize the text into words\n",
    "words = word_tokenize(text)\n",
    "\n",
    "# Lowercasing\n",
    "words_lower = [word.lower() for word in words]\n",
    "\n",
    "# Remove punctuation\n",
    "punctuation_table = str.maketrans('', '', string.punctuation)\n",
    "words_no_punct = [word.translate(punctuation_table) for word in words_lower]\n",
    "\n",
    "# Remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "words_filtered = [word for word in words_no_punct if word not in stop_words]\n",
    "\n",
    "# Print tokenized sentences\n",
    "print(\"Tokenized sentences:\")\n",
    "for sentence in sentences:\n",
    "    print(sentence)\n",
    "\n",
    "# Print tokenized words\n",
    "print(\"\\nTokenized words:\")\n",
    "print(words_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/divyanshu/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/divyanshu/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/divyanshu/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
